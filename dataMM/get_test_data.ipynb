{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b235abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converts raw test data (from DeepSpeare) for verse-final word prediction task \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# DATA_FOLDER_PATH = \"dataMM\"\n",
    "DATA_FOLDER_PATH = \"./\"\n",
    "RAW_FOLDER_PATH = \"raw_from_DeepSpeare\"\n",
    "RAW_TEST_FILE = \"shakestest.txt\" # MODIFY THIS\n",
    "PROCESSED_FOLDER_PATH = \"test_data_processed\"\n",
    "END_SYMBOL = \"<eos>\"\n",
    "\n",
    "# Get pretrained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Placeholders to store\n",
    "#   1) Full lines of sonnets for test = previous verse + next verse\n",
    "#   2) Start index and end index (+1) for the tokens associated with the test word\n",
    "#   3) The test words (answers) \n",
    "full_texts, start_end_ids, last_words = [], [], []\n",
    "\n",
    "masked_verses = []\n",
    "\n",
    "# For each sonnet (each line in the raw test file)\n",
    "for sonnet in open(os.path.join(DATA_FOLDER_PATH, RAW_FOLDER_PATH, RAW_TEST_FILE), 'r', encoding='utf-8'):\n",
    "    # Placeholders to store\n",
    "    #   1) Previous verse   \n",
    "    #   2) The last word in the previous verse\n",
    "    #   3/4) The start and end (+1) index of the tokens associated with the last word\n",
    "    prev_full, prev_last_word, prev_start_id, prev_end_id = None, None, None, None\n",
    "\n",
    "    for line in sonnet.strip().split(END_SYMBOL): # For each verse in the current sonnet\n",
    "        stripped = line.strip() # Remove preceding and trailing spaces \n",
    "        if len(stripped) > 0: # If this line is not empty (artifact of split by `end_symbol`) \n",
    "            words = stripped.split() # Get individual words in the line\n",
    "            last_word = words[-1] # Extract the last word\n",
    "            words[-1] = '_' # Insert a special symbol for start index identification\n",
    "            marked_last_word = \" \".join(words)\n",
    "            words.append(last_word)\n",
    "            marked = \" \".join(words)\n",
    "            start_id = tokenizer.tokenize(marked).index('_') + 1 # Get start index (+1 for [CLS])\n",
    "            tokenized_text = tokenizer.tokenize(stripped)\n",
    "            end_id = len(tokenized_text) + 1          \n",
    "            \n",
    "            if prev_full: # If this is not the first verse\n",
    "                # Concat the previous verse with the current verse to form a test line\n",
    "                full_text_line = prev_full + ' ' + stripped\n",
    "                marked_verse = prev_marked + ' ' + stripped\n",
    "\n",
    "                # Update the placeholders\n",
    "                full_texts.append(full_text_line)\n",
    "                start_end_ids.append([prev_start_id, prev_end_id])\n",
    "                last_words.append(prev_last_word)\n",
    "                masked_verses.append(marked_verse)\n",
    "\n",
    "            prev_marked = marked_last_word\n",
    "            prev_full, prev_last_word, prev_start_id, prev_end_id = stripped, last_word, start_id, end_id\n",
    "\n",
    "\n",
    "# print(prev_full)\n",
    "# print(prev_last_word)\n",
    "# print(prev_start_id)\n",
    "# print(prev_end_id)\n",
    "# print(start_end_ids)\n",
    "# print(full_texts)\n",
    "with open(\"./test_data_processed/test_syllables.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    syllables = f.read()\n",
    "# print(syllables.split(\"\\n\"))\n",
    "\n",
    "syllables = syllables.split(\"\\n\")\n",
    "\n",
    "full_textsyl = []\n",
    "for i in range(len(full_texts)): \n",
    "    full_textsyl.append(full_texts[i] + \" \" + syllables[i])\n",
    "    \n",
    "# input_syllables = tokenizer(syllables)\n",
    "            \n",
    "# Get the maximum number of tokens in each verse\n",
    "# Double that for the max length passed to the tokenizer\n",
    "max_len = np.max([l[1] for l in start_end_ids])\n",
    "\n",
    "# Tokenize the test lines\n",
    "# inputs = tokenizer(full_texts, max_length=max_len * 2, padding='max_length')\n",
    "inputs = tokenizer(full_textsyl, max_length = 89, padding = 'max_length')\n",
    "\n",
    "\n",
    "# # Save the ground truth labels (copy of inputs['input_ids'], not masked yet)\n",
    "# torch.save(torch.tensor(inputs['input_ids'], dtype=torch.int), \n",
    "#            os.path.join(DATA_FOLDER_PATH, PROCESSED_FOLDER_PATH, \"test_labels.pt\"))\n",
    "\n",
    "# Replace test tokens with [MASK] (id=103)\n",
    "for i in range(len(inputs['input_ids'])):\n",
    "    start_id, end_id = start_end_ids[i][0], start_end_ids[i][1]\n",
    "    inputs['input_ids'][i][start_id:end_id] = [tokenizer.mask_token_id] * (end_id-start_id)\n",
    "\n",
    "# Save the individual tensors for testing\n",
    "for key in inputs.keys():\n",
    "    inputs[key] = torch.tensor(inputs[key],dtype=torch.int)\n",
    "    torch.save(inputs[key], os.path.join(DATA_FOLDER_PATH, PROCESSED_FOLDER_PATH, f\"test_{key}.pt\"))\n",
    "\n",
    "# Save the list of test words (ground truths)\n",
    "if not os.path.exists(os.path.join(DATA_FOLDER_PATH, PROCESSED_FOLDER_PATH)):\n",
    "    os.mkdir(os.path.join(DATA_FOLDER_PATH, PROCESSED_FOLDER_PATH))\n",
    "with open(os.path.join(DATA_FOLDER_PATH, PROCESSED_FOLDER_PATH, \"test_last_words.txt\"), 'w') as f:     \n",
    "    for word in last_words:\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "with open(os.path.join(DATA_FOLDER_PATH, PROCESSED_FOLDER_PATH, \"test_masked_verses.txt\"), 'w') as f:     \n",
    "    for verse in masked_verses:\n",
    "        f.write(f\"{verse}\\n\")\n",
    "        \n",
    "with open(os.path.join(DATA_FOLDER_PATH, PROCESSED_FOLDER_PATH, \"test_verses.txt\"), 'w', encoding = \"utf-8\") as f:     \n",
    "    for line in full_texts:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5f750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
